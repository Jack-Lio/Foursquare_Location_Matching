{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Anaconda3\\envs\\ML\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from models.config.defaults import cfg\n",
    "from models.LM import LM\n",
    "from models.data import DATA\n",
    "\n",
    "def inference(model, tokenizer, device, test_data, threshold = 0.5, result_path = './result.csv', batch_size = 32):\n",
    "    \n",
    "    # process test data\n",
    "    text, id_1s, id_2s= test_data['text'], test_data['id_1'], test_data['id_2']\n",
    "        \n",
    "    text_t = []\n",
    "    for i in tqdm(range(len(text))):\n",
    "        tokens = tokenizer(\n",
    "                text[i],\n",
    "                add_special_tokens=True,\n",
    "                padding = 'max_length',\n",
    "                truncation = True,\n",
    "                return_offsets_mapping = False,\n",
    "                max_length = cfg.DATA.PREPROCESS_MAX_LEN,\n",
    "                return_token_type_ids = False,\n",
    "                return_attention_mask = False,\n",
    "                return_tensors = 'pt',\n",
    "            )['input_ids'].tolist()\n",
    "        text_t.append(tokens)\n",
    "    print(len(text_t))\n",
    "    batch_size = min(batch_size, len(text_t))\n",
    "    text_t = torch.tensor(text_t).reshape(len(text_t), -1)\n",
    "\n",
    "    # id2int : conver id to integer \n",
    "    id2int = {}\n",
    "    int2id = list(set(id_1s+id_2s))\n",
    "    for i, id_1 in enumerate(int2id):\n",
    "        id2int[id_1] = i\n",
    "\n",
    "    # convert id_1s and id_2s to integer\n",
    "    id_1s = torch.tensor([id2int[id_1] for id_1 in id_1s]).reshape(-1, 1)\n",
    "    id_2s = torch.tensor([id2int[id_2] for id_2 in id_2s]).reshape(-1, 1)\n",
    "\n",
    "    bg = DataLoader(TensorDataset(text_t, id_1s, id_2s), batch_size = batch_size, shuffle = False)\n",
    "    \n",
    "    dict_ans = {ent:[ent] for ent in int2id}\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(text_t)) as pbar:\n",
    "            for idx, (text, id_1, id_2) in enumerate(bg):\n",
    "                text = text.to(device)\n",
    "                id_1 = id_1.to(device)\n",
    "                id_2 = id_2.to(device)\n",
    "                predict_res = model.predict(text, threshold = threshold)\n",
    "                # filter the id_1s and id_2s which get predict_res as 1\n",
    "                id_1_t = id_1[predict_res == 1]\n",
    "                id_2_t = id_2[predict_res == 1]\n",
    "\n",
    "                pbar.update(batch_size)\n",
    "\n",
    "                # record the result \n",
    "                for i in range(len(id_1_t)):\n",
    "                    dict_ans[int2id[id_1_t[i]]].append(int2id[id_2_t[i]])\n",
    "    \n",
    "    for key in dict_ans.keys():\n",
    "        print(key, dict_ans[key])\n",
    "    print('result saved in {}'.format(result_path))          \n",
    "    with open(result_path, 'w') as f:\n",
    "        for ent in dict_ans:\n",
    "            f.write(ent + ',' + ' '.join(dict_ans[ent]) + '\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data..., data path:  ../dataset\n",
      "test_data:  5\n",
      "generating test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<?, ?it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 5026.73it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 2498.69it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 2504.06it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 2141.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data_list:  15\n",
      "organizing test_data_list as dictionary format: {'text': text, 'num_entities': num_entities}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 14965.40it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 499.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [06:40<00:00, 26.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_000020eb6fed40 ['E_000020eb6fed40', 'E_00002f98667edf', 'E_00001118ad0191', 'E_001b6bad66eb98']\n",
      "E_001b6bad66eb98 ['E_001b6bad66eb98', 'E_0283d9f61e569d', 'E_00001118ad0191', 'E_00002f98667edf']\n",
      "E_00002f98667edf ['E_00002f98667edf', 'E_000020eb6fed40', 'E_00001118ad0191', 'E_001b6bad66eb98']\n",
      "E_0283d9f61e569d ['E_0283d9f61e569d', 'E_001b6bad66eb98', 'E_00001118ad0191', 'E_00002f98667edf']\n",
      "E_00001118ad0191 ['E_00001118ad0191', 'E_001b6bad66eb98', 'E_0283d9f61e569d', 'E_00002f98667edf']\n",
      "result saved in ../dataset/test_result.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cfg.MODEL.IS_TRAIN = False\n",
    "cfg.DATA.DATA_SAVED = False\n",
    "cfg.MODEL.DEVICE = 'cpu'\n",
    "cfg.TEST.MODEL_PATH = '../checkpoints/20220610_xlm-roberta-base/model_1.pth'\n",
    "\n",
    "# set the device\n",
    "if cfg.MODEL.DEVICE == \"cpu\":\n",
    "    device = torch.device(cfg.MODEL.DEVICE)\n",
    "else:\n",
    "    device = torch.device(cfg.MODEL.DEVICE  if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load model\n",
    "model = LM(cfg)\n",
    "model.load_model(cfg.TEST.MODEL_PATH)\n",
    "\n",
    "# set the model to device\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.MODEL.PRETRAINED_MODEL_PATH)\n",
    "transformer = AutoModelForMaskedLM.from_pretrained(\n",
    "    cfg.MODEL.PRETRAINED_MODEL_PATH, \n",
    "    output_hidden_states=True, \n",
    "    output_attentions=True).to(device)\n",
    "for param in transformer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# load data \n",
    "D = DATA(cfg)\n",
    "test_data = D.get_test_data_dict(auto_gen=True, features = cfg.DATA.FEATURES[:-1], rounds = cfg.TEST.ROUNDS, n_neighbors=cfg.TEST.N_NEIGHBORS)\n",
    "\n",
    "# inference on test dataset\n",
    "inference(model, tokenizer, device, test_data, threshold = cfg.TEST.BEST_THRESHOLD, result_path = cfg.TEST.RESULT_PATH, batch_size = cfg.TEST.BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de0bc8409ef025c1cbb840425cc59fe64b9a9f2a4a8c73e6bb75ee7bfb1e593c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
